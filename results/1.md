# command line

./bin/llama-speculative-simple -m  ../../models/Llama-3.2-3B-Instruct-Q5_K_M.gguf -md ../../models/Llama-3.2-3B-Instruct-IQ3_M.gguf -f test.txt -c 0 --color

--------------------------------------------------

# prompt 

Wed Nov 12 11:28:58 UTC 2025: cat test.txt \
what is your name?

--------------------------------------------------

# output

```
<|begin_of_text|>what is your name? -the famous quote
The phrase "What is your name?" is famously associated with the character of the Queen of Hearts in Lewis Carroll's 1865 novel "Alice'
s Adventures in Wonderland". In the story, the Queen of Hearts is known for shouting "Off with their heads!" and she also asks Alice "
What's your name?" when she encounters her.

The Queen of Hearts is a tyrannical ruler and the main antagonist of the story. She is infamous for her short temper and her habit of
beheading those who displease her.

The phrase has since become a popular cultural reference, often used humorously or ironically to ask someone about their identity or t
o express surprise or annoyance.

Interestingly, in the original book, the Queen of Hearts asks Alice "What's your name?" rather than "What is your name?", making the p
hrase even more distinctive.
```

--------------------------------------------------

# llama.cpp logs

```
encoded    6 tokens in    0.533 seconds, speed:   11.263 t/s
decoded  174 tokens in   32.247 seconds, speed:    5.396 t/s

n_draft   = 16
n_predict = 174
n_drafted = 139
n_accept  = 107
accept    = 76.978%

draft:

llama_perf_context_print:        load time =    7130.31 ms
llama_perf_context_print: prompt eval time =   10698.73 ms /    78 tokens (  137.16 ms per token,     7.29 tokens per second)
llama_perf_context_print:        eval time =   11622.56 ms /   102 runs   (  113.95 ms per token,     8.78 tokens per second)
llama_perf_context_print:       total time =   32782.04 ms /   180 tokens
llama_perf_context_print:    graphs reused =        102

target:

llama_perf_sampler_print:    sampling time =      22.38 ms /   174 runs   (    0.13 ms per token,  7776.19 tokens per second)
llama_perf_context_print:        load time =    7883.82 ms
llama_perf_context_print: prompt eval time =   11315.02 ms /   211 tokens (   53.63 ms per token,    18.65 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =   39912.39 ms /   212 tokens
llama_perf_context_print:    graphs reused =         27
llama_memory_breakdown_print: | memory breakdown [MiB] | total   free     self   model   context   compute    unaccounted |
llama_memory_breakdown_print: |   - Host               |                 16951 =  2207 +   14336 +     408                |
```

<!-- vim: nofoldenable
