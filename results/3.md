# command line

./bin/llama-simple -m ../../models/Llama-3.2-3B-Instruct-Q5_K_M.gguf $(cat test.txt)

--------------------------------------------------

# prompt 

Wed Nov 12 11:28:58 UTC 2025: cat test.txt \
how does speculative execution work?

--------------------------------------------------

# output

```
<|begin_of_text|>how does speculative execution work? and what are the its implications?
Speculative execution is a technique used by modern CPUs to improve performance by executing instructions before they are known to be needed. This
```

--------------------------------------------------

# llama.cpp logs

```
main: decoded 32 tokens in 3.16 s, speed: 10.14 t/s

llama_perf_sampler_print:    sampling time =       4.19 ms /    32 runs   (    0.13 ms per token,  7629.95 tokens per second)
llama_perf_context_print:        load time =     792.50 ms
llama_perf_context_print: prompt eval time =     350.45 ms /     7 tokens (   50.06 ms per token,    19.97 tokens per second)
llama_perf_context_print:        eval time =    2792.61 ms /    31 runs   (   90.08 ms per token,    11.10 tokens per second)
llama_perf_context_print:       total time =    3599.22 ms /    38 tokens
llama_perf_context_print:    graphs reused =         30
```

<!-- vim: nofoldenable
