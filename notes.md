      Accelerating Large Language Model Decoding with Speculative Sampling
               (Auto-regressive Sampling vs Speculative Sampling)
                     URL: https://arxiv.org/pdf/2302.01318

                                === Problem ===
Transformer decoding remains a highly costly and inefficient process in this regime. Since each new token depends on the past, many such transformer calls are required to sample a new sequence. Whilst transformers can be trained efficiently and in parallel on TPUs and GPUs, samples are typically drawn auto-regressively. For most applications, auto-regressive sampling (ArS) is highly memory bandwidth bound and thus cannot make effective use of modern accelerator hardware (Shazeer, 2019). A memory bound model call only generates a single token for every sequence in the batch, hence generating multiple tokens introduces a large amount of latency in any system which makes use of it.

                                === Solution ===
Speculative Sampling (SpS): an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. (SIMD)

                               === Algorithm ===
[1] Generate a short draft of length K (draft model)
[2] Score the draft using the lager model (i.e. the model from we wish to sample from - target model)
[3] Accept a subset of K from left to right (via a rejection sampling scheme) recovering the distribution of the target model in the process.

                                 ===  Notes ===
[1] The next token might sometimes be “obvious” therefore if there is strong agreement between the draft and target model’s distributions on a given token or sub-sequence of tokens, this setup permits the generation of multiple tokens each time the target model is called.
[2] The latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target mode

                              ===  Conclusion ===
We show that the expected acceptance rate of draft tokens is sufficient to offset the overhead of the drafting process for large language models (LLMs), resulting in an effective and practical method for reducing sampling latency without the need for modifying the target model or biasing the sample distribution.


                               === RESOURCES ===
             https://github.com/hemingkx/SpeculativeDecodingPapers
                        https://arxiv.org/abs/2302.01318
                        https://arxiv.org/abs/2308.04623
                 https://news.ycombinator.com/item?id=43216518
                        https://arxiv.org/abs/2106.04970
        https://pytorch.org/blog/hitchhikers-guide-speculative-decoding
       https://research.google/blog/looking-back-at-speculative-decoding
                        https://arxiv.org/abs/2408.15766
                 https://news.ycombinator.com/item?id=37390024
                 https://news.ycombinator.com/item?id=37357783
      https://saibo-creator.github.io/post/2024_03_08_speculative_sampling
               https://github.com/ggml-org/llama.cpp/issues/2030
                https://github.com/ggml-org/llama.cpp/pull/2926
                  https://github.com/ollama/ollama/issues/5800
                 https://ralphmao.github.io/ML-software-system
            https://huggingface.co/blog/whisper-speculative-decoding


// vim: nonu
