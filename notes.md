# [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/pdf/2302.01318)
> [!TIP]
> auto-regressive sampling vs speculative sampling

## The problem
Transformer decoding remains a highly costly and inefficient process in this regime. Since each new token depends on the past, many such transformer calls are required to sample a new sequence. Whilst transformers can be trained efficiently and in parallel on TPUs and GPUs, samples are typically drawn auto-regressively. For most applications, auto-regressive sampling (ArS) is highly memory bandwidth bound and thus cannot make effective use of modern accelerator hardware (Shazeer, 2019). A memory bound model call only generates a single token for every sequence in the batch, hence generating multiple tokens introduces a large amount of latency in any system which makes use of it.

## Solution
Speculative Sampling (SpS): an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call.

## Algorithm
[1] Generate a short draft of length K (draft model)
[2] Score the draft using the lager model (i.e. the model from we wish to sample from - target model)
[3] Accept a subset of K from left to right (via a rejection sampling scheme) recovering the distribution of the target model in the process.

## Notes
[1] The next token might sometimes be “obvious” therefore if there is strong agreement between the draft and target model’s distributions on a given token or sub-sequence of tokens, this setup permits the generation of multiple tokens each time the target model is called.
[2] The latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target mode

## Choice of Draft Models
[1] Incorporating draft generation into the target model and train the model from the start.
[2] Using sequence level distillation to generate a second model which predicts K tokens in parallel.
[3] Set a portion of the activations of the target model as an input to the draft model, and train the draft model with this input.

## Conclusion
We show that the expected acceptance rate of draft tokens is sufficient to offset the overhead of the drafting process for large language models (LLMs), resulting in an effective and practical method for reducing sampling latency without the need for modifying the target model or biasing the sample distribution.


# Resources
1. https://github.com/hemingkx/SpeculativeDecodingPapers
2. https://arxiv.org/abs/2302.01318
3. https://arxiv.org/abs/2308.04623
4. https://news.ycombinator.com/item?id=43216518
5. https://arxiv.org/abs/2106.04970
6. https://pytorch.org/blog/hitchhikers-guide-speculative-decoding
7. https://research.google/blog/looking-back-at-speculative-decoding
8. https://arxiv.org/abs/2408.15766
9. https://news.ycombinator.com/item?id=37390024
10. https://news.ycombinator.com/item?id=37357783
11. https://saibo-creator.github.io/post/2024_03_08_speculative_sampling
12. https://github.com/ggml-org/llama.cpp/issues/2030
13. https://github.com/ggml-org/llama.cpp/pull/2926
14. https://github.com/ollama/ollama/issues/5800
15. https://ralphmao.github.io/ML-software-system
16. https://huggingface.co/blog/whisper-speculative-decoding
17. https://github.com/ggml-org/llama.cpp/pull/10362
18. https://arxiv.org/abs/2211.17192
19. https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/speculative_decoding.ipynb#scrollTo=af0b3757-72dc-48a8-9d9d-fc135386cae5

# Footnote
1. `logit(p) = log(p/(1-p))` is the raw, denormalized predictions generated by a model before applying any activation function

<!-- vim: nonu spell hls
